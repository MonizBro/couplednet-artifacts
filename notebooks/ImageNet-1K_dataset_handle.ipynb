{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0fa13-be59-4f34-b1c6-200456284949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import scipy.io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83fe02-307c-4757-8f40-c05f3a82ef00",
   "metadata": {},
   "source": [
    "## Convert meta.mat to synsets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734304f-08d5-4839-8b93-26d44a1e8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert .mat to .csv\n",
    "# Load the .mat file\n",
    "mat_data = scipy.io.loadmat('data/meta.mat')\n",
    "\n",
    "# Check the keys in the .mat file to identify the data variable\n",
    "print(mat_data.keys())\n",
    "\n",
    "# Extract the relevant variable from the .mat file\n",
    "synsets = mat_data['synsets']\n",
    "\n",
    "# Extract the fields from the structured array and convert them to a list of dictionaries\n",
    "data = []\n",
    "for i in range(synsets.shape[0]):\n",
    "    synset_info = synsets[i, 0]  # Each entry is a struct, so we extract it\n",
    "    synset_data = {\n",
    "        'ILSVRC2012_ID': synset_info['ILSVRC2012_ID'][0],\n",
    "        'WNID': synset_info['WNID'][0],\n",
    "        'words': synset_info['words'][0],\n",
    "        'gloss': synset_info['gloss'][0],\n",
    "        'num_children': synset_info['num_children'][0],\n",
    "        'children': synset_info['children'][0],\n",
    "        'wordnet_height': synset_info['wordnet_height'][0],\n",
    "        'num_train_images': synset_info['num_train_images'][0]\n",
    "    }\n",
    "    data.append(synset_data)\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as a CSV file\n",
    "df.to_csv('data/synsets.csv', index=False)  # Save the DataFrame to a .csv file\n",
    "print('meta.mat successfully converted to synsets.csv')\n",
    "\n",
    "# The validation folder has all images in one folder and a val_annotations.txt file with bounding box annotations (filename, class label, x/y coordinates, height, width).\n",
    "val_data = pd.read_csv(f'data/synsets.csv')\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe1454-a410-4bbc-8f05-77bdf7f4408c",
   "metadata": {},
   "source": [
    "## Dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c77dc-f7ba-4b23-9e9c-ffb80377319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training, validation, testing data paths directly in the current directory\n",
    "TRAIN_DIR = os.path.join('data', 'train', 'zipped_1000')\n",
    "VALID_DIR = os.path.join('data', 'val')\n",
    "TEST_DIR = os.path.join('data', 'test')\n",
    "\n",
    "# Print the paths to confirm\n",
    "print(f\"Training data path: {TRAIN_DIR}\")\n",
    "print(f\"Validation data path: {VALID_DIR}\")\n",
    "print(f\"Validation data path: {TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6664b5-35ee-4067-ad76-3a1d867f2334",
   "metadata": {},
   "source": [
    "## Train Dataset - Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b84c0d-29b0-4471-80b6-adf04283dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store  all the 1000 WNIDs in train dataset\n",
    "WNID = []\n",
    "# Get the list of image filenames in the directory and sort them in ascending order\n",
    "file_names = sorted(os.listdir(TRAIN_DIR))\n",
    "\n",
    "# Exatract all the 1000 zipped files according to their WNIDs\n",
    "for file_name in file_names:\n",
    "    # Check if the file is a .tar file\n",
    "    if file_name.endswith('.tar'):\n",
    "        # Extract the WNID by removing the .tar extension\n",
    "        wnid = file_name.replace('.tar', '')\n",
    "        WNID.append(wnid)\n",
    "        # Rename folder name considering their WNIDs\n",
    "        tar_file_path = os.path.join(TRAIN_DIR, file_name)\n",
    "        extract_folder = os.path.join('data', 'train', 'unzipped_1000', wnid)\n",
    "        # Create the folder with WNID as its name\n",
    "        if not os.path.exists(extract_folder):\n",
    "            os.makedirs(extract_folder)\n",
    "            # Extract the folder\n",
    "            with tarfile.open(tar_file_path, 'r') as tar_ref:\n",
    "                tar_ref.extractall(extract_folder)\n",
    "\n",
    "print(f\"Extracted all 1000 classes in training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e1ac6-30e2-4b89-885d-c4dd43f24f42",
   "metadata": {},
   "source": [
    "## Train Dataset - ClassIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38cd80e-4dac-49d2-950c-569ec00ddada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synsets file path\n",
    "synsets_file_path = 'data/synsets.csv'\n",
    "# Load the synsets.csv using pandas\n",
    "synsets_df = pd.read_csv(synsets_file_path)\n",
    "# Filter only the relevant columns (ILSVRC2012_ID, WNID, words)\n",
    "synsets_df = synsets_df[['ILSVRC2012_ID', 'WNID', 'words']]\n",
    "# Remove []-brakets in ILSVRC2012_ID\n",
    "synsets_df['ILSVRC2012_ID'] = synsets_df['ILSVRC2012_ID'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "# Filter the rows where WNID is in the list extracted earlier\n",
    "train_df = synsets_df[synsets_df['WNID'].isin(WNID)]\n",
    "# Sort the DataFrame by the 'WNID' column in ascending order\n",
    "train_df_sorted = train_df.sort_values(by='WNID', ascending=True)\n",
    "# Add a new column 'Class Label' with values from 1 to the length of the DataFrame\n",
    "train_df_sorted['Class Label'] = range(0, len(train_df_sorted))\n",
    "# Save the filtered data to a new CSV file\n",
    "train_csv_path = 'data/classes_train.csv'\n",
    "train_df_sorted.to_csv(train_csv_path, index=False)\n",
    "\n",
    "print(f\"Saved all the Training dataset class information to {train_csv_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c223207-0fa1-45f8-830c-835c95cdf720",
   "metadata": {},
   "source": [
    "## Validation Dataset - ClassIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357aee5-ec84-4e21-96df-d8b93ae4179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the validation ILSVRC2012_IDs\n",
    "val_ILSVRC2012_ID = []\n",
    "# Open the file and read the numbers line by line\n",
    "with open('data/ILSVRC2012_validation_ground_truth.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Convert each line to an integer and append it to the list\n",
    "        val_ILSVRC2012_ID.append(str(line.strip()))  # .strip() removes any extra whitespace or newline characters\n",
    "\n",
    "# Filter the rows where val_ILSVRC2012_ID is in the list extracted earlier\n",
    "val_df = synsets_df[synsets_df['ILSVRC2012_ID'].isin(val_ILSVRC2012_ID)]\n",
    "# Sort the DataFrame by the 'WNID' column in ascending order\n",
    "val_df_sorted = val_df.sort_values(by='WNID', ascending=True)\n",
    "# Add a new column 'Class Label' with values from 1 to the length of the DataFrame\n",
    "val_df_sorted['Class Label'] = range(0, len(val_df_sorted))\n",
    "# Save the filtered data to a new CSV file\n",
    "val_csv_path = 'data/classes_val.csv'\n",
    "val_df_sorted.to_csv(val_csv_path, index=False)\n",
    "\n",
    "print(f\"Saved all the Validation dataset class information to {val_csv_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00867dfb-3c3a-4e9f-9ef4-8fcff48ad3cd",
   "metadata": {},
   "source": [
    "## Create val_annotation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e044ea-0752-46e0-9e6a-f8e21af22be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the validation Image Names ans Class label\n",
    "val_images = []\n",
    "val_class = []\n",
    "# Get the list of image filenames in the directory and sort them in ascending order\n",
    "image_names = sorted(os.listdir(VALID_DIR))\n",
    "\n",
    "# Extract all validation image names\n",
    "for img_name in image_names:\n",
    "    # Check if the file is a .JPEG file\n",
    "    if img_name.endswith('.JPEG'):\n",
    "        val_images.append(img_name)\n",
    "\n",
    "print(f\"Extracted all image names in validation dataset.\")\n",
    "\n",
    "# Open the file and read the numbers line by line\n",
    "with open('data/ILSVRC2012_validation_ground_truth.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        val_class.append(line.strip())\n",
    "\n",
    "print(f\"Extracted all image labels in validation dataset.\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open('data/val_annotation.txt', 'w') as f:\n",
    "    # Iterate through the image names & label lists\n",
    "    for item1, item2 in zip(val_images, val_class):\n",
    "        # Filter the DataFrame by the given ILSVRC2012_ID (item2)\n",
    "        wnid, label = val_df_sorted.loc[val_df_sorted['ILSVRC2012_ID'] == item2, ['WNID', 'Class Label']].iloc[0]\n",
    "        # item2 - 1, to match with the \n",
    "        f.write(f'{item1}, {wnid}, {label}\\n')\n",
    "\n",
    "print(f\"val_annotation.txt created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
